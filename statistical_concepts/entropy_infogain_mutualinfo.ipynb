{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f33ebf",
   "metadata": {},
   "source": [
    "# Entropy, Information Gain, Mutual Information\n",
    "\n",
    "Outline:\n",
    "1. What is Surprise / Information + some examples\n",
    "2. Entropy and how it connects with Surprise / Information\n",
    "3. Information Gain, how it's used in decision tree classification\n",
    "4. Mutual Information\n",
    "5. Measures of divergence between two probability distributions: Kullback-Leibler (Relative entropy) and Jensen-Shannon\n",
    "6. Calculating Mutual Information using KL Divergence, proof it yields the same result with using Entropy\n",
    "7. An example for Mutual Information\n",
    "\n",
    "Summary:\n",
    "- Surprise is the logged inverse of probability. Also called Information, knowing about rarer events has more value compared to knowing about more common events\n",
    "- Entropy is the expected value of surprise\n",
    "- Entropy is a measure of heterogeneity\n",
    "    - Thus reducing entropy is important in Decision Trees to have a classifier **that aims to completely separate** class A from class B\n",
    "- Information gain is the expected reduction in entropy after a change in the dataset. For decision tree classification, the \"change\" is splitting the dataset into subsets\n",
    "- Divergence measures can be also used for evaluating how good a distribution approximates another distribution\n",
    "- Mutual information is the information a random variable conveys about the other. In other words, mutual dependence\n",
    "    - It can be calculated using the KL Divergence of the joint distribution and the product of the marginals as well\n",
    "\n",
    "Sources and additional reading:  \n",
    "Josh Starmer - StatQuest for the definition of entropy  \n",
    "[Shannon's Information Theory Paper](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)  \n",
    "Jason Browlee - Machine Learning Mastery for information gain  \n",
    "[Dr. Yao Xie - Entropy and Mutual Information](https://www2.isye.gatech.edu/~yxie77/ece587/Lecture2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3482151",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1038e91",
   "metadata": {},
   "source": [
    "## What is \"surprise\"?\n",
    "\n",
    "Say we have three patios A, B, C containing blue and orange chickens. Numbers are:  \n",
    "A: 6 Orange, 1 Blue  \n",
    "B: 1 Orange, 10 Blue  \n",
    "C: 7 Orange, 7 Blue\n",
    "\n",
    "Picking an orange chicken from A wouldn't be a surprise. Picking a blue chicken from A would be relatively surprising.  \n",
    "For C, both are equally likely, so choosing either would not be surprising.\n",
    "\n",
    "**Surprise is somewhat the inverse of probability**\n",
    "\n",
    "It is also called Shannon Information / Self-Information /  Information. Intuition behind the Information Theory says, **learning a rare event is more informative than learning a common event.** Rare events are more uncertain / more surprising, so they require more information to represent them than common events.  \n",
    "\n",
    "Given there's a one-in-a-million chance of your friend Tina winning the lottery, knowing that Tina won the lottery is definitely more informative than knowing that Tina lost the lottery. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da0035",
   "metadata": {},
   "source": [
    "### Calculating Surprise\n",
    "\n",
    "We said surprise is the inverse of probability, so can we say `Surprise = 1 / Probability`?  **No.**  \n",
    "Imagine a coin flip experiment resulting in Heads every time, `Surprise(Heads) = 1 / P(Heads) = 1` doesn't make sense. We would expect it to be 0.\n",
    "\n",
    "What about **`Surprise = log_b(1 / Probability)`**, where b = 2 if we measure entropy in bits?  \n",
    "This makes `Surprise(Heads) = 0`, and `Surprise(Tails) = Undefined` for the experiment above. \"Undefined\" for Tails is OK, because it's the surprise of something that never happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67154caf",
   "metadata": {},
   "source": [
    "### Example: One coin toss\n",
    "\n",
    "Given P(Heads) = 0.9 and P(Tails) = 0.1, calculating surprise for both:\n",
    "\n",
    "`Surprise(Heads) = log2(1 / P(Heads)) = 0.15`  \n",
    "`Surprise(Tails) = log2(1 / P(Tails)) = 3.32`\n",
    "\n",
    "### Example: Sequence of coin tosses\n",
    "\n",
    "Given the probabilities above, calculate the surprise for a HHT outcome:\n",
    "\n",
    "`Surprise(HHT) = log2(1 / (P(Heads) * P(Heads) * P(Tails))) = - (log2(P(Heads)) + log2(P(Heads)) + log2(P(Tails)))`  \n",
    "` = 3.62`\n",
    "\n",
    "Observe that the **surprise of the sequence is the sum of the surprises for each individual experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7661f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01e385",
   "metadata": {},
   "source": [
    "## How Surprise connects to Entropy\n",
    "\n",
    "If we wanted to calculate the entire surprise of the coin, we would do `Surprise(Heads) * P(Heads) + Surprise(Tails) * P(Tails)`   \n",
    "The \"entire surprise of the coin\" is Entropy, **Entropy is the expected value of surprise.**\n",
    "\n",
    "Thus Entropy = `-sum(P(x) * log2(P(x)) for x € X)`. Notice how the log2 of inverse-probability is simplified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ea540",
   "metadata": {},
   "source": [
    "### Example: Patios A, B, C\n",
    "\n",
    "The entropies are respectively 0.59, 0.44, 1.0.  \n",
    "`Entropy = P(Orange) * Surprise(Orange) + P(Blue) * Surprise(Blue)`  \n",
    "`Entropy(A) = (0.86 * 0.22 + 0.14 * 2.80)`  \n",
    "`Entropy(B) = (0.09 * 3.46 + 0.91 * 0.14)`  \n",
    "`Entropy(C) = (0.50 * 1.00 + 0.50 * 1.00)`\n",
    "\n",
    "**Interpreting the results:**\n",
    "\n",
    "Entropy(A) < Entropy(B): B has a lower probability for the more surprising outcome  \n",
    "Entropy(C) = 1: Even the surprise values are relatively moderate, high probability for both made the entropy 1.  \n",
    "**Entropy increases when heterogeneity increases**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c947e51",
   "metadata": {},
   "source": [
    "### Joint Entropy\n",
    "\n",
    "Nothing new, calculated from the definition:  \n",
    "`H(X, Y) = -sum(P(x, y) * log2(P(x, y)) for x € X, y € Y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88c3e1",
   "metadata": {},
   "source": [
    "### Conditional Entropy\n",
    "\n",
    "Entropy of a rv given another rv.\n",
    "\n",
    "If `(X, Y) ~ P(x, y)` then    \n",
    "`H(Y | X) = sum(P(x, y) * log2(P(x) / P(x, y)) for x € X, y € Y)`\n",
    "\n",
    "Proof:  \n",
    "`H(Y | X) = sum(P(x) * H(Y | X = x) for x € X)`  \n",
    "`H(Y | X) = -sum(P(x) * sum(P(y | x) * log2(P(y | x)) for y € Y) for x € X)`  \n",
    "`H(Y | X) = -sum(P(x, y) * log2(P(y | x)) for x € X, y € Y)` by Chain rule  \n",
    "`H(Y | X) = -sum(P(x, y) * log2(P(x, y) / P(x)) for x € X, y € Y)`  \n",
    "`H(Y | X) = sum(P(x, y) * log2(P(x) / P(x, y)) for x € X, y € Y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4829db2",
   "metadata": {},
   "source": [
    "### Chain Rule for Entropy\n",
    "\n",
    "Entropy of a pair of rvs: Entropy of one + conditional entropy of the other:  \n",
    "`H(Y | X) = H(X, Y) - H(X)`  \n",
    "`H(X | Y) = H(X, Y) - H(Y)`\n",
    "\n",
    "Proof:  \n",
    "`H(Y | X) = -sum(P(x, y) * log2(P(x, y))) + -sum(P(x) * log2(1 / P(x))`  \n",
    "`H(Y | X) = -sum(P(x, y) * log2(P(x, y) / P(x)))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6368d67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c104a3",
   "metadata": {},
   "source": [
    "## Information Gain (IG)\n",
    "\n",
    "**Informal Definition:** Expected reduction in entropy after a change in the dataset. For decision tree classification, the \"change\" may be splitting a dataset depending on the value of a feature\n",
    "\n",
    "By definition, Information Gain is: `IG(S, a) = H(S) - H(S | a)` in \"bits\" units. S is the dataset, a is the feature.\n",
    "\n",
    "The conditional entropy `H(S | a)` is the expected value of entropies after the split.  \n",
    "Say we split S into subsets S1, ..., SN depending on some value v of a.  \n",
    "`H(S | a) = sum(|Si| / |S| * H(Si))`\n",
    "\n",
    "**Information Gain is also used in ID3 algorithm. If the Information Gain is smaller than some value, the algorithm stops splitting at that node.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087e1ce",
   "metadata": {},
   "source": [
    "### Calculating Information Gain\n",
    "\n",
    "Say we have a dataset S of 13 Orange, 7 Blue chicken. Our algorithm splits this dataset into two different subsets S1, S2. One subset has 7 Orange, 1 Blue chicken; the other has 6 Orange, 6 Blue chicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683299d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of S: 0.93407 bits, Entropy of S1: 0.54356 bits, Entropy of S2: 1.0 bits\n",
      "Weighted sum of entropies after the split: 0.81743 bits\n",
      "Information Gain: 0.11664 bits\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(subset0, subset1):\n",
    "    # for binary classification\n",
    "    return -(subset0 * log2(subset0) + subset1 * log2(subset1))\n",
    "\n",
    "def information_gain(num_s, entropy_s, num_s1, entropy_s1, num_s2, entropy_s2):\n",
    "    # for binary classification\n",
    "    return entropy_s - ((num_s1 / num_s) * entropy_s1 + (num_s2 / num_s) * entropy_s2)\n",
    "    \n",
    "\n",
    "# entropy of the dataset S before the split\n",
    "entropy_S = entropy(13 / 20, 7 / 20)\n",
    "\n",
    "# entropies of subsets S1, S2 after the split\n",
    "entropy_S1 = entropy(7 / 8, 1 / 8)\n",
    "entropy_S2 = entropy(6 / 12, 6 / 12)\n",
    "\n",
    "# information gain\n",
    "IG = information_gain(20, entropy_S, 8, entropy_S1, 12, entropy_S2)\n",
    "\n",
    "print(f\"Entropy of S: {round(entropy_S, 5)} bits, Entropy of S1: {round(entropy_S1, 5)} bits, \"\n",
    "      f\"Entropy of S2: {round(entropy_S2, 5)} bits\")\n",
    "print(f\"Weighted sum of entropies after the split: {round(-IG + entropy_S, 5)} bits\")\n",
    "print(f\"Information Gain: {round(IG, 5)} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280a205a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599b997",
   "metadata": {},
   "source": [
    "## Mutual Information (MI)\n",
    "\n",
    "**Informal Definition:** Average reduction in uncertainty for random variable (rv) X that results from knowing rv Y. In other words, the average amount of information that X conveys about Y.\n",
    "\n",
    "By definition, Mutual Information is: `I(X; Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)` in \"bits\" units.   \n",
    "**Observe that it is the same with Information Gain.**  \n",
    "\n",
    "By the chain rule, `I(X; Y) = H(X) + H(Y) - H(X, Y)` is also valid.\n",
    "\n",
    "MI is a measure of dependence (mutual dependence) between two rvs. So MI is symmetrical, `I(X; Y) = I(Y; X)`.\n",
    "\n",
    "\n",
    "**A nice Venn diagram showing what Mutual Information is:**\n",
    "![](mutual_info.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c102397e",
   "metadata": {},
   "source": [
    "### What is Relative Entropy a.k.a Kullback-Leiber (KL) Divergence?\n",
    "\n",
    "KL Divergence quantifies how much one probability distribution differs from another.  \n",
    "`KL(P || Q)` denotes the P's divergence from Q.\n",
    "\n",
    "`KL(P || Q) = sum(P(x) * log2(P(x) / Q(x)) for x € X)`\n",
    "\n",
    "Intuition is: When probability for an event from P is large, but the probability for the same event in Q is small, there is a large divergence. When probability from P is small and the probability from Q is large, there is also a large divergence, but not as large as the first case.\n",
    "\n",
    "KL divergence is not symmetrical, `KL(P || Q) != KL(Q || P)`\n",
    "\n",
    "If we're to approximate an unknown probability distribution, then the target dist from the data is P, and Q is the approximation of P. In this case, **KL score is the average number of additional bits to represent an event from the random variable.** Better the approximation, lower the KL score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a28720",
   "metadata": {},
   "source": [
    "#### An empirical example for KL Divergence\n",
    "\n",
    "We have a rv X with three events `red, green, blue`. There are two probability distributions for this variable.   \n",
    "`PMF(P) = [red: 0.10, green: 0.40, blue: 0.50]`, and `PMF(Q) = [red: 0.80, green: 0.15, blue: 0.05]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506448d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the two distributions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_events = [\"red\", \"green\", \"blue\"]  # random variable's events\n",
    "P = np.array([0.10, 0.40, 0.50])\n",
    "Q = np.array([0.80, 0.15, 0.05])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Distributions P and Q for X_events')\n",
    "\n",
    "ax1.set_title(\"P\")\n",
    "ax1.bar(X_events, P)\n",
    "ax1.set_yticks([num / 10 for num in range(11)])\n",
    "\n",
    "ax2.set_title(\"Q\")\n",
    "ax2.bar(X_events, Q)\n",
    "ax2.set_yticks([num / 10 for num in range(11)])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47103756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we approximate P with Q: KL(P || Q) = 1.92698 bits\n",
      "If we approximate Q with P: KL(Q || P) = 2.02165 bits\n"
     ]
    }
   ],
   "source": [
    "# calculating the KL Divergence for P and Q\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * log2(p[i] / q[i]) for i in range(len(p)) if p[i] > 0 and q[i] > 0)\n",
    "\n",
    "print(f\"If we approximate P with Q: KL(P || Q) = {round(kl_divergence(P, Q), 5)} bits\")\n",
    "print(f\"If we approximate Q with P: KL(Q || P) = {round(kl_divergence(Q, P), 5)} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50edf814",
   "metadata": {},
   "source": [
    "KL(Q || P) is obviously higher, because of the case \"If the prob for Q is high and prob for P is low, it's a larger difference than prob for P is high and prob for Q is low\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62250538",
   "metadata": {},
   "source": [
    "### Jenssen-Shannon (JS) Divergence\n",
    "\n",
    "The normalized and symmetrical form of KL Divergence.\n",
    "\n",
    "`JS(P || Q) = 0.5 * KL(P || (P + Q) / 2) + 0.5 * KL(Q || (P + Q) / 2)`\n",
    "\n",
    "When the base-2 log is used, it gives a divergence score between 0 (identical) and 1 (totally different).\n",
    "\n",
    "The square-root of the JS score is the JS distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb22d",
   "metadata": {},
   "source": [
    "**Applying JS Divergence to the above example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d29571f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JS divergence: JS(P || Q) = JS(Q || P) = 0.4202 bits\n",
      "JS distance: 0.64823\n"
     ]
    }
   ],
   "source": [
    "def js_divergence(p, q):\n",
    "    m = (p + q) / 2\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "print(f\"JS divergence: JS(P || Q) = JS(Q || P) = {round(js_divergence(P, Q), 5)} bits\")\n",
    "print(f\"JS distance: {round(js_divergence(P, Q) ** 0.5, 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e14f7",
   "metadata": {},
   "source": [
    "### Calculating MI with KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1cb11",
   "metadata": {},
   "source": [
    "We can calculate MI as the KL Divergence (Relative entropy) of the joint distribution and the product of independent dists. If the KL score is zero, the two dists are totally independent.\n",
    "\n",
    "`I(X; Y) = KL(P(x, y) || P(x) * P(y)) = sum(P(x, y) * log2(P(x, y) / (P(x) * P(y)) for x € X, y € Y)`\n",
    "\n",
    "This yields the same result with the Entropy-related approach.  \n",
    "`I(X; Y) = -sum(P(x, y) * log2(P(x) * P(y) / P(x, y))`  \n",
    "`I(X; Y) = H(P(x) * P(y)) - H(P(x, y))`  \n",
    "`I(X; Y) = H(P(x)) + H(P(y)) - H(P(x, y))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca675b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a88c96",
   "metadata": {},
   "source": [
    "## An empirical example combining everything upto now\n",
    "\n",
    "Story: We have the probabilities of blood type + skin cancer risk. Find the Mutual Information between the two.\n",
    "\n",
    "X: Blood type, Y: Skin cancer risk\n",
    "\n",
    "| Skin Cancer Risk | A | B | AB | O |\n",
    "| ----------- | - | - | -- | - |\n",
    "| Very Low | 1/8 | 1/16 | 1/32 | 1/32 |\n",
    "| Low | 1/16 | 1/8 | 1/32 | 1/32 |\n",
    "| Medium | 1/16 | 1/16 | 1/16 | 1/16 |\n",
    "| High | 1/4 | 0 | 0 | 0 |\n",
    "\n",
    "Marginal Probabilities:\n",
    "\n",
    "| Blood Type | Probability |\n",
    "| ---------- | ----------- |\n",
    "| A | 1/2 |\n",
    "| B| 1/4 |\n",
    "| AB | 1/8 |\n",
    "| O | 1/8 |\n",
    "\n",
    "| Skin Cancer Risk | Probability |\n",
    "| ---------- | ----------- |\n",
    "| Very Low | 1/4 |\n",
    "| Low | 1/4 |\n",
    "| Medium | 1/4 |\n",
    "| High | 1/4 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27c3bb",
   "metadata": {},
   "source": [
    "Let's find H(X), H(Y), H(X | Y), H(Y | X), H(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabe0288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X) = 1.75 bits\n",
      "H(Y) = 2.0 bits\n",
      "H(X, Y) = 3.375 bits\n",
      "H(X | Y) = 1.375 bits\n",
      "H(Y | X) = 1.625 bits\n",
      "I(X; Y) = 0.375 bits\n"
     ]
    }
   ],
   "source": [
    "X = {\"A\": 1 / 2, \"B\": 1 / 4, \"AB\": 1 / 8, \"O\": 1 / 8}\n",
    "Y = {\"Very Low\": 1 / 4, \"Low\": 1 / 4, \"Medium\": 1 / 4, \"High\": 1 / 4}\n",
    "\n",
    "XY = np.array([\n",
    "    1 / 8, 1 / 16, 1 / 32, 1 / 32,\n",
    "    1 / 16, 1 / 8, 1 / 32, 1 / 32,\n",
    "    1 / 16, 1 / 16, 1 / 16, 1 / 16,\n",
    "    1 / 4, 0, 0, 0\n",
    "])\n",
    "\n",
    "X_product_Y = np.array([x * y for x in X.values() for y in Y.values()])\n",
    "\n",
    "\n",
    "def entropy(probs: list):\n",
    "    return -sum(p * log2(p) for p in probs if p > 0)\n",
    "\n",
    "print(f\"H(X) = {round(entropy(X.values()), 5)} bits\")\n",
    "print(f\"H(Y) = {round(entropy(Y.values()), 5)} bits\")\n",
    "print(f\"H(X, Y) = {round(entropy(XY), 5)} bits\")\n",
    "print(f\"H(X | Y) = {round(entropy(XY) - entropy(Y.values()), 5)} bits\")\n",
    "print(f\"H(Y | X) = {round(entropy(XY) - entropy(X.values()), 5)} bits\")\n",
    "print(f\"I(X; Y) = {round(entropy(X.values()) + entropy(Y.values()) - entropy(XY), 5)} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3ca08",
   "metadata": {},
   "source": [
    "This is in compliance with the Venn diagram above as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
